::: {.content-hidden unless-format="html"}
$$
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\plim}{\operatornamewithlimits{plim}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Exp}{\mathrm{E}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\irow}[1]{%
\begin{pmatrix}#1\end{pmatrix}
}
$$
:::

# Exercises III

### Required packages {.unnumbered}

```{r, message = FALSE, warning = FALSE, results = 'hide'}
pkgs <- c("sf", "mapview", "spdep", "spatialreg", "ggplot2", "tmap", "viridis", "viridisLite", 
          "plm", "lfe", "splm", "SDPDmod")
lapply(pkgs, require, character.only = TRUE)

```

### Session info {.unnumbered}

```{r}
sessionInfo()

```


### Reload data from pervious session {.unnumbered}

```{r}
load("_data/msoa2_spatial.RData")
```



## Environmental inequality (continued)

Let's use the same neighbours weights definition as before:

```{r}
coords <- st_centroid(msoa.spdf)

# Neighbours within 3km distance
dist_15.nb <- dnearneigh(coords, d1 = 0, d2 = 2500)

summary(dist_15.nb)

# There are some empty neighbour sets. Lets impute those with the nearest neighbour.
k2.nb <- knearneigh(coords, k = 1)

# Replace zero
nolink_ids <- which(card(dist_15.nb) == 0)
dist_15.nb[card(dist_15.nb) == 0] <- k2.nb$nn[nolink_ids, ]

summary(dist_15.nb)

# listw object with row-normalization
dist_15.lw <- nb2listw(dist_15.nb, style = "W")
```

and estiamte the spatial SAR model: 

```{r}
mod_1.sar <- lagsarlm(log(no2) ~ per_mixed + per_asian + per_black + per_other
                      + per_nonUK_EU + per_nonEU  + log(POPDEN),  
                      data = msoa.spdf, 
                      listw = dist_15.lw,
                      Durbin = FALSE) # we could here extend to SDM
summary(mod_1.sar)
```


### 1) Please calculate the true multiplier matrix of this SAR model. {.unnumbered}

The multiplier matrix is given by $({\bm I_N}-\rho {\bm W})^{-1}$.

```{r}
W <- listw2mat(dist_15.lw)
I <- diag(dim(W)[1])

rho <- unname(mod_1.sar$rho)

M <- solve(I - rho*W)

M[1:10, 1:10]
```


### 2) Create an N x N effects matrix for the effect of the non-EU citizens. What is the effect of unit 6 on unit 10? Why is this larger than the effect of unit 5 on unit 8? {.unnumbered}


```{r}
# For beta 1

beta <- mod_1.sar$coefficients

effM <- beta["per_nonEU"] * M

effM[1:10, 1:10]

# "Effect" of unit 6 on unit 10
effM[10, 6]

# "Effect" of unit 5 on unit 8
effM[8, 5]
```


### 3) Calculate and interpret the summary impact measures of the SAR model. {.unnumbered}


```{r}
mod_1.sar.imp <- impacts(mod_1.sar, listw = dist_15.lw, R = 300)
summary(mod_1.sar.imp)
```


### 4) Is SAR the right model choice or would you rather estimate a different model? Please run a Durbin model and caculate its impact summary measures {.unnumbered}


```{r}
# Spatial Dubrbin model
mod_1.durb <- lagsarlm(log(no2) ~ per_mixed + per_asian + per_black + per_other
                      + per_nonUK_EU + per_nonEU + log(POPDEN),  
                      data = msoa.spdf, 
                      listw = dist_15.lw,
                      Durbin = TRUE)

summary(mod_1.durb)

# Impact measures of the Durbin Error model
mod_1.durb.imp <- impacts(mod_1.durb, listw = dist_15.lw, R = 300)
summary(mod_1.durb.imp, zstats = TRUE, short = TRUE)
```


### 5) Please repeat with a Durbin Error model. Why are the impacts here idenptical to the coefficients? {.unnumbered}

```{r}
# Spatial Dubrbin model
mod_1.durbe <- errorsarlm(log(no2) ~ per_mixed + per_asian + per_black + per_other
                      + per_nonUK_EU + per_nonEU + log(POPDEN),  
                      data = msoa.spdf, 
                      listw = dist_15.lw,
                      Durbin = TRUE)

summary(mod_1.durbe)

# Impact measures of the Durbin model
mod_1.durbe.imp <- impacts(mod_1.durbe, listw = dist_15.lw, R = 300)
summary(mod_1.durbe.imp, zstats = TRUE, short = TRUE)
```



## Inkar data: the effect of regional characteristics on life expectancy

Below, we read and transform some characteristics of the [INKAR data](https://www.inkar.de/) on German counties.


<!-- ```{r} -->
<!-- di <- c("_data/") -->

<!-- # Define the downloaded filed -->
<!-- j <- c("inkar.csv") -->
<!-- c <- 1 -->

<!-- for(k in j){ -->
<!--   header <- as.vector(t(read.table(paste0(di, k), nrows = 1, sep = ";")[1,])) -->
<!--   # Clean header -->
<!--   header <- stringi::stri_replace_all_fixed( -->
<!--     header,  -->
<!--     c("ä", "ö", "ü", "Ä", "Ö", "Ü"),  -->
<!--     c("ae", "oe", "ue", "Ae", "Oe", "Ue"),  -->
<!--     vectorize_all = FALSE -->
<!--   ) -->
<!--   header <- gsub(" ", "", header) -->
<!--   header <- gsub("\\.", "", header) -->
<!--   header <- iconv(header, "latin1", "ASCII", sub = "") -->

<!--   # Combine with second row header (year) -->
<!--   header2 <- as.vector(t(read.table(paste0(di, k), skip = 1, nrows = 1, sep = ";")[1,])) -->
<!--   header3 <- paste(header, header2, sep = "_") -->
<!--   header3 <- gsub("_NA", "", header3) -->

<!--   nc <- length(header3) -->
<!--   # Input and rename data -->
<!--   data <- read.csv(paste0(di, k), skip = 2, header = FALSE, sep = ";",  -->
<!--                    quote = "\"", dec = ",", stringsAsFactors = F, -->
<!--                    colClasses = "character") -->
<!--   names(data) <- header3 -->
<!--   data1 <- data -->

<!--   # Correct character vars (containing thousands separator) -->
<!--   vars <- which(sapply(data1, is.character)) -->
<!--   vars <- vars[-which(vars %in% c(1:3))] -->
<!--   for(l in vars){ -->
<!--     data1[,l] <- gsub("\\.", "", data1[,l]) -->
<!--     data1[,l] <- gsub("\\,", ".", data1[,l]) -->
<!--     data1[,l] <- as.numeric(data1[,l]) -->
<!--   } -->


<!--   # #Save -->
<!--   # l <- paste("bearb", k, sep = "_") -->
<!--   # write.table(data1, file = l, row.names = FALSE, sep = ";", dec = ".", na = ".") -->

<!--   # #Reshape -->
<!--   # helpvar1 <- unique(header[4:length(header)]) -->
<!--   # helpvar2 <-  sort(unique(header2[!is.na(header2)])) -->
<!--   # n_vars <- length(helpvar1) -->
<!--   # n_times <- length(helpvar2) -->
<!--   # helpvar1 <- sort(rep(helpvar1, times = n_times)) -->
<!--   # helpvar2 <- rep(helpvar2, times = n_vars) -->
<!--   # helpvar3 <- paste(helpvar1, helpvar2, sep = "_") -->
<!--   # count <- ncol(data1)+1 -->
<!--   # for(v in helpvar3) { -->
<!--   #   if(v %in% names(data1)) {} -->
<!--   #   else{ -->
<!--   #     data1[,count] <- NA -->
<!--   #     colnames(data1)[count] <- v -->
<!--   #     count <- count+1 -->
<!--   #   } -->
<!--   # } -->
<!--   # data1 <- data1[c(colnames(data1)[1:3], sort(helpvar3))] -->
<!--   #  -->
<!--   # data1 <- reshape(data1, direction = "long", varying = 4:ncol(data1),  -->
<!--   #                  sep = "_") -->
<!--   data.long <- tidyr::pivot_longer(data1,  -->
<!--                                   cols = 4:ncol(data1), -->
<!--                                   names_to = c(".value", "year"), -->
<!--                                   names_pattern = "(.*)_(.*)") -->


<!--   colnames(data.long) <- substr(colnames(data.long), 1, 30) -->

<!--   if(c == 1){ -->
<!--     inkar.df <- data.long -->
<!--   }else{ -->
<!--     inkar.df <- merge(inkar.df, data.long, all.x = TRUE, all.y = TRUE) -->
<!--   } -->

<!--   c <- c+1 -->

<!-- } -->



<!-- inkar.df$year <- as.numeric(inkar.df$year) -->

<!-- names(inkar.df)[which(names(inkar.df) == "Pkw-Dichte")] <- "pkw_dichte" -->

<!-- save(inkar.df, file = "_data/inkar.Rdata") -->

<!-- ``` -->

```{r}
load("_data/inkar2.Rdata")
```


Variables are

| Variable | Description |
| ------------					   | ------------ |
| "Kennziffer"                      | ID                                         |
| "Raumeinheit"                     | Name                                       |
| "Aggregat"                        | Level                                      |
| "year"                            | Year                                       |
| "poluation_density"               | Population Density       |
| "median_income"                   | Median Household income (only for 2020)                   |
| "gdp_in1000EUR"                   | Gross Domestic Product in 1000 euros                            |
| "unemployment_rate"               | Unemployment rate                            |
| "share_longterm_unemployed"       | Share of longterm unemployed (among unemployed)                               |
| "share_working_indutry"           | Share of employees in undistrial sector                    |
| "share_foreigners"                | Share of foreign nationals                              |
| "share_college"                   | Share of school-finishers with college degree                              |
| "recreational_space"              | Recreational space per inhabitant                           |
| "car_density"                     | Density of cars                                 |
| "life_expectancy"                 | Life expectancy       |


## County shapes

```{r}
kreise.spdf <- st_read(dsn = "_data/vg5000_ebenen_1231",
                       layer = "VG5000_KRS")
```

### 1) Please map the life expectancy across Germany {.unnumbered}

a) Merge data with the shape file (as with conventional data)

```{r}
# Merge
inkar_2020.spdf <- merge(kreise.spdf, inkar.df[inkar.df$year == 2020, ], 
                         by.x = "AGS", by.y = "Kennziffer")
```

b) Create a map of life-expectancy

```{r}
cols <- viridis(n = 100, direction = -1, option = "G")

mp1 <-  ggplot(data = inkar_2020.spdf) +
  geom_sf(aes(fill = life_expectancy), color = "white", size = 0.5) +
  scale_fill_gradientn(
    colours = cols,  # your custom palette
    name = "in years",
    na.value = "grey90"
  ) +
  labs(title = "Life expectancy") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 8),
    legend.background = element_rect(fill = "white", color = "black"),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

mp1

```

### 2) Chose some variables that could predict life expectancy. See for instance the [following paper](https://doi.org/10.1073/pnas.2003719117). {.unnumbered}


### 3) Generate a neighbours object (e.g. the 10 nearest neighbours). {.unnumbered}

```{r}
# nb <- poly2nb(kreise.spdf, row.names = "ags", queen = TRUE)
knn <- knearneigh(st_centroid(kreise.spdf), k = 10)
nb <- knn2nb(knn, row.names = kreise.spdf$ags)
listw <- nb2listw(nb, style = "W")
```


### 4) Estimate a cross-sectional spatial model for the year 2020 and calculate the impacts. {.unnumbered}


```{r}

### Use a spatial Durbin Error model

# Spec formula
fm <- life_expectancy ~ median_income + unemployment_rate + share_college + car_density

# Estimate error model with Durbin = TRUE 
mod_1.durb <- errorsarlm(fm,  
                      data = inkar_2020.spdf, 
                      listw = listw,
                      Durbin = TRUE)

summary(mod_1.durb)

# Calculate impacts (which is unnecessary in this case)
mod_1.durb.imp <- impacts(mod_1.durb, listw = listw, R = 300)
summary(mod_1.durb.imp, zstats = TRUE, short = TRUE)

```


### 5) Calculate the spatial lagged variables for your covariates (e.g. use create_WX(), which needs a non-spatial df as input) . {.unnumbered}


```{r}
# Extract covariate names
covars <- attr(terms(fm),"term.labels")

w_vars <- create_WX(st_drop_geometry(inkar_2020.spdf)[, covars],
                    listw = listw,
                    prefix = "w")

inkar_2020.spdf <- cbind(inkar_2020.spdf, w_vars)
```


### 6) Can you run a spatial machine learning model? (for instance, using `randomForest`)? {.unnumbered}

```{r}
library(randomForest)

# Train
rf.mod <- randomForest(life_expectancy ~ median_income + unemployment_rate + share_college + car_density +
                         w.median_income + w.unemployment_rate + w.share_college + w.car_density,
                       data = st_drop_geometry(inkar_2020.spdf), 
                       ntree = 1000,
                       importance = TRUE)

# Inspect the mechanics of the model
importance(rf.mod)
varImpPlot(rf.mod)
```

You could even go further and use higher order neighbours (e.g. `nblag(queens.nb, maxlag = 3)`) to check the importance of direct neighbours and the neighbours neighbours and so on ...




## Esimate an FE model with SLX specification


a) Loops over years to generate WX

```{r}

# We use gdp instead of median income (which is only available in recent year)
fm <- life_expectancy ~ gdp_in1000EUR + unemployment_rate + share_college + car_density

# All years where we have a balanced sample
years <- unique(inkar.df$year[which(complete.cases(inkar.df[, all.vars(fm)]))])

# All variables we want ot lag
vars <- all.vars(fm)

# create listw with the correct rownames (ID = Kennziffer)
kreise.spdf$Kennziffer <- kreise.spdf$ags
knn <- knearneigh(st_centroid(kreise.spdf), k = 10)
nb <- knn2nb(knn, row.names = kreise.spdf$Kennziffer)
listw <- nb2listw(nb, style = "W")

for(y in years){
  # Select singe year
  tmp <- inkar.df[inkar.df$year == y ,]
  # Select variables and make df
  x <- st_drop_geometry(tmp[, vars])
  # Add ID as rownames (we retreive them again later)
  rownames(x) <- tmp$Kennziffer
  # Perform lag transformation (rownames contian ids)
  w.tmp <- create_WX(as.matrix(x),
                    listw = listw,
                    prefix = "w",
                    zero.policy = TRUE) # NAs will get zero
  w.tmp <- as.data.frame(w.tmp)
  
  # add indices back
  w.tmp$Kennziffer <- row.names(w.tmp)
  w.tmp$year <- y
  
  if(y == years[1]){
    w.inkar.df <- w.tmp
  }else{
    w.inkar.df <- rbind(w.inkar.df, w.tmp)
  }
}

head(w.inkar.df)

# Merge back 

inkar.df <- merge(inkar.df, w.inkar.df, by = c("Kennziffer", "year"))

```


b) Estimate a twoways FE SLX panel model

```{r}
slx.fe <- felm(life_expectancy ~ gdp_in1000EUR + unemployment_rate + share_college + car_density +
                 w.gdp_in1000EUR + w.unemployment_rate + w.share_college + w.car_density
                | Kennziffer + year | 0 | Kennziffer,
              data = inkar.df)

summary(slx.fe)
```

c) Estimate a twoways FE SAR panel model (use `spml()`)

```{r}
### Estimate model
sar.fe <- spml(life_expectancy ~ gdp_in1000EUR + unemployment_rate + share_college + car_density, 
               data = inkar.df, 
               index = c("Kennziffer", "year"), 
               listw = listw, 
               model= "within",
               effect= "twoways",
               lag = TRUE, 
               spatial.error = "none"
               )

summary(sar.fe)
```

d) Estimate the summary impacts.

```{r}
sar.fe.imp <- impacts(sar.fe, listw = listw, time = length(years), R = 200)
summary(sar.fe.imp, zstats = TRUE, short = TRUE)
```


<!-- We use the `WDI` API package to retrieve data from the World Bank. -->

<!-- You can open the [World Bank Data browser](https://databank.worldbank.org/home.aspx) to go though the data. -->

<!-- You can search for indicators with `WDIsearch()`. -->
<!-- ```{r warning=FALSE} -->
<!-- library(WDI) -->

<!-- # Search GDP per capita -->
<!-- WDIsearch("CO2 intensity") -->

<!-- # Political Stability -->
<!-- WDIsearch("Political Stability") -->

<!-- #  -->
<!-- WDIsearch("democracy") -->
<!-- # The Democracy indicator is an additive eleven-point scale (0-10) -->

<!-- ``` -->

<!-- ```{r, cache=TRUE} -->
<!-- # Define countries, indicators to query, and time period -->
<!-- wd.df <- WDI(country = "all",  -->
<!--              indicator = c('population' = "SP.POP.TOTL",  -->
<!--                            'gdp_pc' = "NY.GDP.PCAP.KD",  -->
<!--                            'co2_pc' = "EN.ATM.CO2E.PC", -->
<!--                            'co2_intesity' = "EN.ATM.CO2E.EG.ZS", -->
<!--                            'gini' = "SI.POV.GINI", -->
<!--                            'political_stability' = "GV.POLI.ST.ES", -->
<!--                            'inst_democr' = "UPP.INS.DEMO.XQ"), -->
<!--              extra = TRUE, -->
<!--              start = 2010, end = 2019) -->

<!-- # Save -->
<!-- save(wd.df, file = "_data/WDI_data.RData") -->
<!-- ``` -->


<!-- ## Diffusion of political regimes -->

<!-- See for instance @Gleditsch.2006 for an example for the diffusion of democratization. -->




